{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249cb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import snscrape.modules.twitter as sntwitter \n",
    "import datetime \n",
    "from tqdm.notebook import tqdm_notebook \n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e928797",
   "metadata": {},
   "source": [
    "This code imports the necessary libraries for data manipulation and mining from various sources.\n",
    "•\tThe pandas library is used for data manipulation and analysis.\n",
    "•\tThe numpy library is used for numerical calculations and statistical analysis.\n",
    "•\tThe snscrape library provides a simple way to access data from social media platforms like Twitter.\n",
    "•\tThe datetime library provides classes for working with dates and times.\n",
    "•\tThe tqdm library is used to display a progress bar to keep track of the program's execution.\n",
    "•\tThe apyori library is used for association rule learning and mining.\n",
    "By importing these libraries, the program can utilize the functions and methods that are defined within each library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eff7cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query text to be matched (or leave it blank by pressing enter)Federal Bank\n",
      "Enter startdate in this format yyyy-mm-dd (or leave it blank by pressing enter): 2023-01-01\n",
      "Enter enddate in this format yyyy-mm-dd (or leave it blank by pressing enter): 2023-03-31\n",
      "Enter max number of tweets or enter -1 to retrieve all possible tweets: -1\n"
     ]
    }
   ],
   "source": [
    "# Get user input for search query, start and end dates, and max number of tweets to retrieve\n",
    "text = input('Enter query text to be matched (or leave it blank by pressing enter)') \n",
    "since = input('Enter startdate in this format yyyy-mm-dd (or leave it blank by pressing enter): ') \n",
    "until = input('Enter enddate in this format yyyy-mm-dd (or leave it blank by pressing enter): ') \n",
    "count = int(input('Enter max number of tweets or enter -1 to retrieve all possible tweets: ')) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6c31685",
   "metadata": {},
   "source": [
    "This code prompts the user to input values for a search query, start and end dates, and a maximum number of tweets to retrieve.\n",
    "•\tinput() is a built-in Python function that prompts the user to enter a value.\n",
    "•\ttext is a variable that will store the user's input for the search query.\n",
    "•\tsince is a variable that will store the user's input for the start date in the format yyyy-mm-dd.\n",
    "•\tuntil is a variable that will store the user's input for the end date in the format yyyy-mm-dd.\n",
    "•\tcount is a variable that will store the user's input for the maximum number of tweets to retrieve, or -1 if the user wants to retrieve all possible tweets.\n",
    "These variables will be used later to filter and retrieve Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a5d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the search query using the user input\n",
    "def search(text,since,until): \n",
    "    global filename \n",
    "    q = text\n",
    "    # If end date is not specified, use today's date as the end date\n",
    "    if until=='':\n",
    "        until = datetime.datetime.strftime(datetime.date.today(), '%Y-%m-%d')\n",
    "    q += f\" until:{until}\"\n",
    "    # If start date is not specified, use 7 days before the end date as the start date\n",
    "    if since=='':\n",
    "        since = datetime.datetime.strftime(datetime.datetime.strptime(until, '%Y-%m-%d') - datetime.timedelta(days=7), '%Y-%m-%d')\n",
    "    q += f\" since:{since}\"\n",
    "    # Create a filename for the output file based on the search query and dates\n",
    "    filename = f\"{since}_{until}_{text}.csv\"\n",
    "    print(filename)\n",
    "    return q "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5db5ef3b",
   "metadata": {},
   "source": [
    "This is Python code that defines a function search() which takes three arguments, text, since, and until.\n",
    "\n",
    "The purpose of this function is to create a search query for Twitter data based on user input for the search query, start date, and end date. It also creates a filename for the output file based on the search query and dates.\n",
    "\n",
    "•\tglobal filename declares that filename is a global variable that can be accessed and modified throughout the program.\n",
    "•\tq is a variable that stores the search query text specified by the user.\n",
    "•\tIf the until variable is empty (i.e., the user did not specify an end date), it sets until to today's date using the datetime module and adds it to the search query.\n",
    "•\tIf the since variable is empty (i.e., the user did not specify a start date), it sets since to 7 days before the end date using the datetime module and adds it to the search query.\n",
    "•\tThe filename variable is created by concatenating the start date, end date, and search query text with underscores to create a CSV file name.\n",
    "•\tThe function returns the search query q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15267895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-01_2023-03-31_Federal Bank.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc4ba8b6ea74a0bab3e97ac84ccea98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the search function and store the resulting query in the 'q' variable\n",
    "q = search(text, since, until)\n",
    " \n",
    "tweets_list1 = [] \n",
    "\n",
    "# If the user has chosen to retrieve all tweets, use a progress bar to show progress\n",
    "if count == -1: \n",
    "    for i, tweet in enumerate(tqdm_notebook(sntwitter.TwitterSearchScraper(q).get_items())): \n",
    "        hashtags = tweet.hashtags\n",
    "        # Add the hashtags from each tweet to the 'tweets_list1' list\n",
    "        if hashtags is not None:\n",
    "            tweets_list1.append(hashtags) \n",
    "# If the user has specified a limit on the number of tweets to retrieve, use a progress bar to show progress\n",
    "else: \n",
    "    with tqdm_notebook(total=count) as pbar: \n",
    "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper(q).get_items()): \n",
    "            # Break the loop if the maximum number of tweets has been reached\n",
    "            if i >= count:  \n",
    "                break \n",
    "            hashtags = tweet.hashtags\n",
    "            # Add the hashtags from each tweet to the 'tweets_list1' list\n",
    "            if hashtags is not None:\n",
    "                tweets_list1.append(hashtags) \n",
    "            # Update the progress bar\n",
    "            pbar.update(1) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "876e65fa",
   "metadata": {},
   "source": [
    "This code calls the search() function to create a search query based on the user input and stores the resulting query in the q variable.\n",
    "\n",
    "The code then creates an empty list called tweets_list1 to store the hashtags extracted from each tweet.\n",
    "\n",
    "If the user has chosen to retrieve all tweets (count == -1), the code uses the tqdm_notebook() function to create a progress bar and iterates over each tweet returned by the sntwitter.TwitterSearchScraper(q).get_items() function.\n",
    "\n",
    "If the user has specified a limit on the number of tweets to retrieve (count is a positive integer), the code uses the tqdm_notebook() function to create a progress bar with a total count of count and iterates over each tweet returned by the sntwitter.TwitterSearchScraper(q).get_items() function, breaking the loop once the maximum number of tweets has been reached.\n",
    "\n",
    "For each tweet, the code extracts the hashtags and appends them to the tweets_list1 list if they are not None. The code also updates the progress bar with each iteration.\n",
    "\n",
    "Overall, this code retrieves tweets containing the search query specified by the user, extracts the hashtags from each tweet, and stores them in a list called tweets_list1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a053c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of hashtags to a pandas dataframe\n",
    "tweets_df1 = pd.DataFrame({'Hashtags': tweets_list1})\n",
    "\n",
    "# Remove any rows where the 'Hashtags' column is empty\n",
    "tweets_df1 = tweets_df1.dropna(subset=['Hashtags'])\n",
    "\n",
    "# Convert each list of hashtags in the 'Hashtags' column to a set to remove duplicates, then convert it back to a list and store it in a new list called 'Hashtags'\n",
    "Hashtags = tweets_df1['Hashtags'].apply(lambda x: list(set(x))).tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47ef5132",
   "metadata": {},
   "source": [
    "This code converts the list of hashtags stored in tweets_list1 to a pandas DataFrame called tweets_df1.\n",
    "\n",
    "The code then removes any rows in the DataFrame where the 'Hashtags' column is empty using the dropna() method.\n",
    "\n",
    "The code next converts each list of hashtags in the 'Hashtags' column to a set using the set() function to remove duplicates. Then it converts the set back to a list using the list() function and stores it in a new list called 'Hashtags'.\n",
    "\n",
    "The resulting 'Hashtags' list contains the unique hashtags extracted from the tweets, with duplicates removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb54bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.05\n",
    "min_confidence = 0.95\n",
    "min_lift = 1.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c537c6f4",
   "metadata": {},
   "source": [
    "These lines define three parameters used in the Apriori algorithm, which is a popular algorithm for association rule mining in data mining. The parameters are:\n",
    "\n",
    "•\tmin_support: the minimum support threshold for an itemset to be considered frequent. Itemsets with support below this threshold will not be considered in the association rule mining process. In this case, it is set to 0.05, which means that itemsets that appear in at least 5% of the transactions will be considered frequent.\n",
    "\n",
    "•\tmin_confidence: the minimum confidence threshold for a rule to be considered strong. Rules with confidence below this threshold will not be included in the final results. In this case, it is set to 0.95, which means that rules with a confidence of at least 95% will be considered strong.\n",
    "\n",
    "•\tmin_lift: the minimum lift threshold for a rule to be considered interesting. Lift is a measure of the strength of the association between two items. Rules with lift below this threshold will not be included in the final results. In this case, it is set to 1.0, which means that only rules with lift greater than or equal to 1 (i.e., indicating a positive association between the items) will be considered interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de105b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the apriori algorithm to generate association rules for the list of hashtags\n",
    "results = list(apriori(Hashtags, \n",
    "                       min_support=min_support, \n",
    "                       min_confidence=min_confidence, \n",
    "                       min_lift=min_lift))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cc87fbc",
   "metadata": {},
   "source": [
    "This code uses the apriori() function from the apyori library to generate association rules for the list of hashtags. The function takes several arguments:\n",
    "\n",
    "•\tHashtags: the list of transaction data to be used in the association rule mining process.\n",
    "•\tmin_support: the minimum support threshold for an itemset to be considered frequent, as defined earlier.\n",
    "•\tmin_confidence: the minimum confidence threshold for a rule to be considered strong, as defined earlier.\n",
    "•\tmin_lift: the minimum lift threshold for a rule to be considered interesting, as defined earlier.\n",
    "\n",
    "The output of the apriori() function is a list of namedtuples representing the generated association rules. Each namedtuple contains the following fields:\n",
    "\n",
    "•\tordered_statistics: a frozenset of ordered statistics for the rule, including the antecedent (left-hand side), consequent (right-hand side), support, confidence, and lift.\n",
    "•\tsupport: the support of the rule (i.e., the proportion of transactions that contain both the antecedent and consequent).\n",
    "•\titemsets: the itemset associated with the rule (i.e., the union of the antecedent and consequent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8540ee77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itemset: MostAdmiredBank, FederalBank\n",
      "Support: 0.061\n",
      "Confidence: 0.993\n",
      "Lift: 2.240\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, FederalBank\n",
      "Support: 0.062\n",
      "Confidence: 1.000\n",
      "Lift: 2.255\n",
      "\n",
      "Itemset: FederalBank, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.966\n",
      "Lift: 2.178\n",
      "\n",
      "Itemset: MostAdmiredBank, Rishta\n",
      "Support: 0.060\n",
      "Confidence: 0.980\n",
      "Lift: 8.670\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, MostAdmiredBank\n",
      "Support: 0.060\n",
      "Confidence: 0.980\n",
      "Lift: 15.820\n",
      "\n",
      "Itemset: MostAdmiredBank, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.579\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, Rishta\n",
      "Support: 0.060\n",
      "Confidence: 0.974\n",
      "Lift: 8.614\n",
      "\n",
      "Itemset: perfectbankingpartner, Rishta\n",
      "Support: 0.058\n",
      "Confidence: 0.980\n",
      "Lift: 8.663\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.966\n",
      "Lift: 15.588\n",
      "\n",
      "Itemset: MostAdmiredBank, FederalBank, Rishta\n",
      "Support: 0.060\n",
      "Confidence: 0.980\n",
      "Lift: 11.769\n",
      "\n",
      "Itemset: MostAdmiredBank, FederalBank, RishtaAapSeHaiSirfAppSeHai\n",
      "Support: 0.060\n",
      "Confidence: 0.980\n",
      "Lift: 15.820\n",
      "\n",
      "Itemset: MostAdmiredBank, FederalBank, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.681\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, FederalBank, Rishta\n",
      "Support: 0.060\n",
      "Confidence: 0.974\n",
      "Lift: 11.693\n",
      "\n",
      "Itemset: FederalBank, perfectbankingpartner, Rishta\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 11.515\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, FederalBank, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.966\n",
      "Lift: 15.588\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, MostAdmiredBank, Rishta\n",
      "Support: 0.060\n",
      "Confidence: 0.980\n",
      "Lift: 16.242\n",
      "\n",
      "Itemset: MostAdmiredBank, perfectbankingpartner, Rishta\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, MostAdmiredBank, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, perfectbankingpartner, Rishta\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: MostAdmiredBank, FederalBank, RishtaAapSeHaiSirfAppSeHai, Rishta\n",
      "Support: 0.060\n",
      "Confidence: 0.980\n",
      "Lift: 16.242\n",
      "\n",
      "Itemset: MostAdmiredBank, FederalBank, perfectbankingpartner, Rishta\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: perfectbankingpartner, MostAdmiredBank, FederalBank, RishtaAapSeHaiSirfAppSeHai\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, FederalBank, perfectbankingpartner, Rishta\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, MostAdmiredBank, perfectbankingpartner, Rishta\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n",
      "Itemset: RishtaAapSeHaiSirfAppSeHai, FederalBank, Rishta, MostAdmiredBank, perfectbankingpartner\n",
      "Support: 0.057\n",
      "Confidence: 0.959\n",
      "Lift: 15.890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the results of the apriori algorithm\n",
    "for r in results:\n",
    "    print(f\"Itemset: {', '.join(r.items)}\")\n",
    "    print(f\"Support: {r.support:.3f}\")\n",
    "    print(f\"Confidence: {r.ordered_statistics[0].confidence:.3f}\")\n",
    "    print(f\"Lift: {r.ordered_statistics[0].lift:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fac81a55",
   "metadata": {},
   "source": [
    "This code prints out the results of the apriori algorithm that was run on the list of hashtags. For each association rule that was generated, it prints out the following information:\n",
    "\n",
    "•\tThe itemset, which is the set of items (in this case, hashtags) that are associated with each other\n",
    "•\tThe support of the itemset, which is the proportion of transactions that contain all of the items in the itemset\n",
    "•\tThe confidence of the association rule, which is the proportion of transactions containing the antecedent (i.e., the items on the left-hand side of the rule) that also contain the consequent (i.e., the items on the right-hand side of the rule)\n",
    "•\tThe lift of the association rule, which is a measure of how much more likely the consequent is given the antecedent, compared to the base rate of the consequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6c4056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MostAdmiredBank -> FederalBank: Support=0.061, Confidence=0.993, Lift=2.240\n",
      "RishtaAapSeHaiSirfAppSeHai -> FederalBank: Support=0.062, Confidence=1.000, Lift=2.255\n",
      "perfectbankingpartner -> FederalBank: Support=0.057, Confidence=0.966, Lift=2.178\n",
      "MostAdmiredBank -> Rishta: Support=0.060, Confidence=0.980, Lift=8.670\n",
      "MostAdmiredBank -> RishtaAapSeHaiSirfAppSeHai: Support=0.060, Confidence=0.980, Lift=15.820\n",
      "perfectbankingpartner -> MostAdmiredBank: Support=0.057, Confidence=0.959, Lift=15.579\n",
      "RishtaAapSeHaiSirfAppSeHai -> Rishta: Support=0.060, Confidence=0.974, Lift=8.614\n",
      "perfectbankingpartner -> Rishta: Support=0.058, Confidence=0.980, Lift=8.663\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai: Support=0.057, Confidence=0.966, Lift=15.588\n",
      "MostAdmiredBank -> FederalBank, Rishta: Support=0.060, Confidence=0.980, Lift=11.769\n",
      "MostAdmiredBank -> RishtaAapSeHaiSirfAppSeHai, FederalBank: Support=0.060, Confidence=0.980, Lift=15.820\n",
      "perfectbankingpartner -> FederalBank, MostAdmiredBank: Support=0.057, Confidence=0.959, Lift=15.681\n",
      "RishtaAapSeHaiSirfAppSeHai -> FederalBank, Rishta: Support=0.060, Confidence=0.974, Lift=11.693\n",
      "perfectbankingpartner -> FederalBank, Rishta: Support=0.057, Confidence=0.959, Lift=11.515\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai, FederalBank: Support=0.057, Confidence=0.966, Lift=15.588\n",
      "MostAdmiredBank -> RishtaAapSeHaiSirfAppSeHai, Rishta: Support=0.060, Confidence=0.980, Lift=16.242\n",
      "perfectbankingpartner -> MostAdmiredBank, Rishta: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai, MostAdmiredBank: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai, Rishta: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "MostAdmiredBank -> RishtaAapSeHaiSirfAppSeHai, FederalBank, Rishta: Support=0.060, Confidence=0.980, Lift=16.242\n",
      "perfectbankingpartner -> FederalBank, MostAdmiredBank, Rishta: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "perfectbankingpartner -> FederalBank, MostAdmiredBank, RishtaAapSeHaiSirfAppSeHai: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai, FederalBank, Rishta: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai, MostAdmiredBank, Rishta: Support=0.057, Confidence=0.959, Lift=15.890\n",
      "perfectbankingpartner -> RishtaAapSeHaiSirfAppSeHai, FederalBank, MostAdmiredBank, Rishta: Support=0.057, Confidence=0.959, Lift=15.890\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    # Extract the antecedent and consequent items from the association rule\n",
    "    antecedent = ', '.join(r.ordered_statistics[0].items_base)\n",
    "    consequent = ', '.join(r.ordered_statistics[0].items_add)\n",
    "    # Extract the support, confidence, and lift values from the association rule\n",
    "    support = r.support\n",
    "    confidence = r.ordered_statistics[0].confidence\n",
    "    lift = r.ordered_statistics[0].lift\n",
    "    # Print the association rule with its support, confidence, and lift values\n",
    "    print(f\"{antecedent} -> {consequent}: Support={support:.3f}, Confidence={confidence:.3f}, Lift={lift:.3f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6f5d3ef",
   "metadata": {},
   "source": [
    "This code takes the results of the apriori algorithm run on a list of hashtags and prints out the association rules. For each association rule, it extracts the antecedent and consequent items, as well as the support, confidence, and lift values. It then formats the association rule and its associated values as a string and prints it to the console. The resulting output displays the antecedent, consequent, support, confidence, and lift values for each association rule in a human-readable format."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd2edab7",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\n",
    "This code implements the Apriori algorithm to generate association rules for a list of hashtags obtained from Twitter data. The user can input a search query, start and end dates, and a maximum number of tweets to retrieve, and the program scrapes Twitter data to obtain the list of hashtags. The Apriori algorithm is then used to identify frequent itemsets and generate association rules with a minimum support, confidence, and lift. The program then prints the association rules with their corresponding support, confidence, and lift values. The output provides insights into the co-occurrence patterns of hashtags on Twitter, which can be useful for marketing, trend analysis, and social network analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
